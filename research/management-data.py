import os
import json
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
import sqlite3
from pathlib import Path
import yaml
import logging
from typing import Dict, List, Any, Optional
import hashlib
import shutil

class ResearchDataManager:
    """
    –°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∞
    """
    
    def __init__(self, project_name: str, base_path: str = "./research_projects"):
        self.project_name = project_name
        self.base_path = Path(base_path)
        self.project_path = self.base_path / project_name
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
        self.setup_project_structure()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.setup_logging()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.setup_metadata_db()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
        self.config = self.load_or_create_config()
        
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–æ–≤–∞–Ω –ø—Ä–æ–µ–∫—Ç: {project_name}")
    
    def setup_project_structure(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫ –ø—Ä–æ–µ–∫—Ç–∞"""
        folders = [
            "data/raw",           # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            "data/processed",     # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            "data/features",      # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            "data/models",        # –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            "analysis",           # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
            "experiments",        # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
            "reports",           # –û—Ç—á–µ—Ç—ã
            "configs",           # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            "logs",              # –õ–æ–≥–∏
            "notebooks",         # Jupyter notebooks
            "scripts",           # –°–∫—Ä–∏–ø—Ç—ã
            "artifacts",         # –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞
            "docs"               # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
        ]
        
        for folder in folders:
            (self.project_path / folder).mkdir(parents=True, exist_ok=True)
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_file = self.project_path / "logs" / f"{self.project_name}_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(f"research_{self.project_name}")
    
    def setup_metadata_db(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        db_path = self.project_path / "metadata.db"
        self.conn = sqlite3.connect(db_path)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS datasets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                file_path TEXT,
                file_hash TEXT,
                size_mb REAL,
                rows INTEGER,
                columns INTEGER,
                created_date TEXT,
                description TEXT,
                source TEXT,
                version TEXT
            )
        ''')
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                dataset_id INTEGER,
                config_path TEXT,
                results_path TEXT,
                model_path TEXT,
                metrics TEXT,
                created_date TEXT,
                status TEXT,
                description TEXT,
                FOREIGN KEY (dataset_id) REFERENCES datasets (id)
            )
        ''')
        
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS features (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT,
                dataset_id INTEGER,
                feature_type TEXT,
                importance REAL,
                description TEXT,
                created_date TEXT,
                FOREIGN KEY (dataset_id) REFERENCES datasets (id)
            )
        ''')
        
        self.conn.commit()
    
    def load_or_create_config(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
        config_path = self.project_path / "configs" / "project_config.yaml"
        
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config = {
                'project': {
                    'name': self.project_name,
                    'created_date': datetime.now().isoformat(),
                    'description': '',
                    'team': [],
                    'version': '1.0.0'
                },
                'data': {
                    'data_sources': [],
                    'preprocessing_steps': [],
                    'feature_engineering': []
                },
                'modeling': {
                    'target_variable': '',
                    'task_type': '',  # classification, regression, clustering
                    'evaluation_metrics': [],
                    'model_types': []
                },
                'deployment': {
                    'target_environment': '',
                    'api_requirements': {},
                    'performance_requirements': {}
                }
            }
            
            self.save_config(config)
            return config
    
    def save_config(self, config: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        config_path = self.project_path / "configs" / "project_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """–†–∞—Å—á–µ—Ç —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def register_dataset(self, 
                        name: str, 
                        file_path: str, 
                        description: str = "", 
                        source: str = "",
                        version: str = "1.0") -> int:
        """
        –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º–µ
        
        Returns:
            dataset_id –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        source_path = Path(file_path)
        
        if not source_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É raw data
        dest_path = self.project_path / "data" / "raw" / source_path.name
        shutil.copy2(source_path, dest_path)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        file_hash = self.calculate_file_hash(dest_path)
        size_mb = dest_path.stat().st_size / (1024 * 1024)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if dest_path.suffix.lower() == '.csv':
            df = pd.read_csv(dest_path)
            rows, columns = df.shape
        else:
            rows, columns = 0, 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        cursor = self.conn.execute('''
            INSERT OR REPLACE INTO datasets 
            (name, file_path, file_hash, size_mb, rows, columns, created_date, description, source, version)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (name, str(dest_path), file_hash, size_mb, rows, columns, 
              datetime.now().isoformat(), description, source, version))
        
        self.conn.commit()
        dataset_id = cursor.lastrowid
        
        self.logger.info(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {name} (ID: {dataset_id})")
        return dataset_id
    
    def save_processed_data(self, 
                          data: pd.DataFrame, 
                          name: str, 
                          processing_steps: List[str],
                          parent_dataset_id: int = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{name}_{timestamp}.parquet"
        file_path = self.project_path / "data" / "processed" / filename
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Parquet (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        data.to_parquet(file_path, compression='snappy')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        metadata = {
            'name': name,
            'file_path': str(file_path),
            'created_date': datetime.now().isoformat(),
            'shape': data.shape,
            'processing_steps': processing_steps,
            'parent_dataset_id': parent_dataset_id,
            'columns': list(data.columns),
            'dtypes': {col: str(dtype) for col, dtype in data.dtypes.items()}
        }
        
        metadata_path = self.project_path / "data" / "processed" / f"{name}_{timestamp}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {filename}")
        return str(file_path)
    
    def save_features(self, 
                     features: pd.DataFrame, 
                     feature_names: List[str],
                     feature_importance: Dict[str, float] = None,
                     dataset_id: int = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"features_{timestamp}.parquet"
        file_path = self.project_path / "data" / "features" / filename
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.to_parquet(file_path, compression='snappy')
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        if dataset_id:
            for feature_name in feature_names:
                importance = feature_importance.get(feature_name, 0.0) if feature_importance else 0.0
                self.conn.execute('''
                    INSERT INTO features (name, dataset_id, feature_type, importance, created_date)
                    VALUES (?, ?, ?, ?, ?)
                ''', (feature_name, dataset_id, 'engineered', importance, datetime.now().isoformat()))
        
        self.conn.commit()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        metadata = {
            'file_path': str(file_path),
            'created_date': datetime.now().isoformat(),
            'feature_names': feature_names,
            'feature_importance': feature_importance or {},
            'shape': features.shape
        }
        
        metadata_path = self.project_path / "data" / "features" / f"features_{timestamp}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏: {filename}")
        return str(file_path)
    
    def save_model(self, 
                  model: Any, 
                  name: str,
                  metrics: Dict[str, float],
                  config: Dict[str, Any],
                  dataset_id: int = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.project_path / "data" / "models" / f"{name}_{timestamp}"
        model_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = model_dir / "model.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
        config_path = model_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = model_dir / "metrics.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        self.conn.execute('''
            INSERT INTO experiments 
            (name, dataset_id, config_path, results_path, model_path, metrics, created_date, status)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (name, dataset_id, str(config_path), str(metrics_path), str(model_path),
              json.dumps(metrics), datetime.now().isoformat(), 'completed'))
        
        self.conn.commit()
        
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {name}")
        return str(model_dir)
    
    def save_analysis_report(self, 
                           report_data: Dict[str, Any], 
                           report_name: str,
                           report_type: str = "analysis") -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{report_name}_{report_type}_{timestamp}.json"
        file_path = self.project_path / "reports" / filename
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –æ—Ç—á–µ—Ç—É
        full_report = {
            'metadata': {
                'report_name': report_name,
                'report_type': report_type,
                'created_date': datetime.now().isoformat(),
                'project_name': self.project_name
            },
            'data': report_data
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç—á–µ—Ç: {filename}")
        return str(file_path)
    
    def create_experiment_config(self, 
                               experiment_name: str,
                               parameters: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{experiment_name}_config_{timestamp}.yaml"
        file_path = self.project_path / "experiments" / filename
        
        config = {
            'experiment': {
                'name': experiment_name,
                'created_date': datetime.now().isoformat(),
                'parameters': parameters,
                'status': 'configured'
            }
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        return str(file_path)
    
    def prepare_production_artifacts(self, 
                                   model_path: str,
                                   feature_config: Dict[str, Any],
                                   api_requirements: Dict[str, Any]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        artifacts_dir = self.project_path / "artifacts" / f"production_{timestamp}"
        artifacts_dir.mkdir(exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        model_source = Path(model_path)
        if model_source.is_file():
            shutil.copy2(model_source, artifacts_dir / "model.pkl")
        elif model_source.is_dir():
            shutil.copytree(model_source, artifacts_dir / "model")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with open(artifacts_dir / "feature_config.json", 'w', encoding='utf-8') as f:
            json.dump(feature_config, f, indent=2, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ API
        with open(artifacts_dir / "api_requirements.json", 'w', encoding='utf-8') as f:
            json.dump(api_requirements, f, indent=2, ensure_ascii=False)
        
        # –°–æ–∑–¥–∞–µ–º README –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞
        readme_content = f"""# Production Artifacts - {self.project_name}

–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().isoformat()}

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
- model.pkl / model/ - –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- feature_config.json - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- api_requirements.json - –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ API

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ model.pkl
2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ feature_config.json
3. –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å API —Å–æ–≥–ª–∞—Å–Ω–æ api_requirements.json
"""
        
        with open(artifacts_dir / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞: {artifacts_dir}")
        return str(artifacts_dir)
    
    def get_project_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É"""
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        datasets = pd.read_sql_query("SELECT * FROM datasets", self.conn)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º
        experiments = pd.read_sql_query("SELECT * FROM experiments", self.conn)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        features = pd.read_sql_query("SELECT * FROM features", self.conn)
        
        summary = {
            'project_info': {
                'name': self.project_name,
                'path': str(self.project_path),
                'config': self.config
            },
            'statistics': {
                'datasets_count': len(datasets),
                'experiments_count': len(experiments),
                'features_count': len(features),
                'total_data_size_mb': datasets['size_mb'].sum() if not datasets.empty else 0
            },
            'recent_activity': {
                'latest_dataset': datasets.iloc[-1].to_dict() if not datasets.empty else None,
                'latest_experiment': experiments.iloc[-1].to_dict() if not experiments.empty else None
            }
        }
        
        return summary
    
    def export_project_structure(self) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        export_path = self.project_path / "docs" / "project_structure.json"
        
        structure = {
            'project_name': self.project_name,
            'created_date': datetime.now().isoformat(),
            'summary': self.get_project_summary(),
            'file_structure': self._get_directory_structure(self.project_path)
        }
        
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(structure, f, indent=2, ensure_ascii=False, default=str)
        
        return str(export_path)
    
    def _get_directory_structure(self, path: Path, max_depth: int = 3, current_depth: int = 0) -> Dict:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        if current_depth >= max_depth:
            return {}
        
        structure = {}
        try:
            for item in path.iterdir():
                if item.is_dir():
                    structure[item.name] = {
                        'type': 'directory',
                        'contents': self._get_directory_structure(item, max_depth, current_depth + 1)
                    }
                else:
                    structure[item.name] = {
                        'type': 'file',
                        'size_bytes': item.stat().st_size
                    }
        except PermissionError:
            pass
        
        return structure
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if hasattr(self, 'conn'):
            self.conn.close()

# –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
class IntegratedResearchPipeline:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, project_name: str):
        self.data_manager = ResearchDataManager(project_name)
        self.logger = self.data_manager.logger
    
    def run_dataset_analysis_pipeline(self, 
                                    dataset_paths: List[str],
                                    dataset_names: List[str] = None):
        """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        if not dataset_names:
            dataset_names = [f"Dataset_{i+1}" for i in range(len(dataset_paths))]
        
        all_results = {}
        
        for i, (path, name) in enumerate(zip(dataset_paths, dataset_names)):
            self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: {name}")
            
            try:
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
                dataset_id = self.data_manager.register_dataset(
                    name=name,
                    file_path=path,
                    description=f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –¥–∞—Ç–∞—Å–µ—Ç #{i+1}",
                    source="Kaggle"
                )
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∞—à –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä)
                # analyzer = KaggleDatasetAnalyzer(path, name)
                # analysis_results = analyzer.run_full_analysis()
                
                # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
                analysis_results = {
                    'dataset_id': dataset_id,
                    'basic_info': {'shape': (1000, 10), 'memory_usage_mb': 5.2},
                    'quality_score': 85,
                    'ml_potential': ['classification', 'regression']
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
                report_path = self.data_manager.save_analysis_report(
                    report_data=analysis_results,
                    report_name=f"{name}_analysis",
                    report_type="dataset_analysis"
                )
                
                all_results[name] = {
                    'dataset_id': dataset_id,
                    'analysis_results': analysis_results,
                    'report_path': report_path
                }
                
                self.logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: {name}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {name}: {e}")
                continue
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        summary_report = {
            'total_datasets': len(all_results),
            'successful_analyses': len([r for r in all_results.values() if 'analysis_results' in r]),
            'datasets_summary': all_results,
            'project_summary': self.data_manager.get_project_summary()
        }
        
        self.data_manager.save_analysis_report(
            report_data=summary_report,
            report_name="datasets_analysis_summary",
            report_type="summary"
        )
        
        return all_results

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def demo_research_data_management():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    print("üî¨ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–ò–ú–ò –î–ê–ù–ù–´–ú–ò")
    print("="*70)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    manager = ResearchDataManager("kaggle_datasets_analysis")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
    summary = manager.get_project_summary()
    print(f"\nüìä –°–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–æ–µ–∫—Ç—É:")
    print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {summary['project_info']['name']}")
    print(f"–ü—É—Ç—å: {summary['project_info']['path']}")
    print(f"–î–∞—Ç–∞—Å–µ—Ç–æ–≤: {summary['statistics']['datasets_count']}")
    print(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {summary['statistics']['experiments_count']}")
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    structure_path = manager.export_project_structure()
    print(f"\nüìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞: {structure_path}")
    
    print("\n" + "="*70)
    print("üìñ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ:")
    print("="*70)
    print("""
1. –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:
   dataset_id = manager.register_dataset('name', 'path/to/file.csv', 'description')

2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
   manager.save_processed_data(df, 'processed_name', ['step1', 'step2'])

3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
   manager.save_features(features_df, feature_list, importance_dict)

4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:
   manager.save_model(model, 'model_name', metrics_dict, config_dict)

5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø—Ä–æ–¥–∞–∫—à–Ω—É:
   manager.prepare_production_artifacts(model_path, feature_config, api_requirements)
    """)
    
    manager.close()

if __name__ == "__main__":
    demo_research_data_management()