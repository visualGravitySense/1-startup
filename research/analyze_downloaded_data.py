#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
–ë—ã—Å—Ç—Ä—ã–π –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
"""

import pandas as pd
import numpy as np
from pathlib import Path
import os

# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
DATA_DIR = Path("data/raw")

def analyze_dataset(file_path, dataset_name):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
    try:
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑: {dataset_name}")
        print("-" * 50)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
        if file_path.suffix.lower() == '.csv':
            df = pd.read_csv(file_path)
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            print(f"‚ö†Ô∏è  –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_path.suffix}")
            return None
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üìè –†–∞–∑–º–µ—Ä: {df.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
        print(f"üíæ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
        
        # –°—Ç–æ–ª–±—Ü—ã
        print(f"\nüìã –°—Ç–æ–ª–±—Ü—ã ({len(df.columns)}):")
        for i, col in enumerate(df.columns[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            print(f"  {i}. {col}")
        if len(df.columns) > 10:
            print(f"  ... –∏ –µ—â–µ {len(df.columns) - 10} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        text_cols = df.select_dtypes(include=['object']).columns
        
        print(f"\nüî¢ –ß–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {len(numeric_cols)}")
        print(f"üìù –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {len(text_cols)}")
        
        # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        missing = df.isnull().sum()
        if missing.sum() > 0:
            print(f"\n‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {missing.sum():,} ({missing.sum()/len(df)*100:.1f}%)")
        else:
            print(f"\n‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")
        
        # –ü–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
        print(f"\nüëÄ –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:")
        print(df.head(3).to_string())
        
        return {
            'name': dataset_name,
            'shape': df.shape,
            'size_kb': file_path.stat().st_size / 1024,
            'numeric_cols': len(numeric_cols),
            'text_cols': len(text_cols),
            'missing_values': missing.sum(),
            'columns': list(df.columns)
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {dataset_name}: {e}")
        return None

def find_datasets():
    """–ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã"""
    datasets = []
    
    if not DATA_DIR.exists():
        print("‚ùå –ü–∞–ø–∫–∞ data/raw –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return datasets
    
    for category_dir in DATA_DIR.iterdir():
        if category_dir.is_dir():
            category = category_dir.name
            print(f"\nüìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            
            for dataset_dir in category_dir.iterdir():
                if dataset_dir.is_dir():
                    dataset_name = dataset_dir.name
                    
                    # –ò—â–µ–º CSV/Excel —Ñ–∞–π–ª—ã
                    data_files = list(dataset_dir.glob("*.csv")) + \
                                list(dataset_dir.glob("*.xlsx")) + \
                                list(dataset_dir.glob("*.xls"))
                    
                    if data_files:
                        print(f"  ‚úÖ {dataset_name} ({len(data_files)} —Ñ–∞–π–ª–æ–≤)")
                        for file_path in data_files:
                            datasets.append({
                                'category': category,
                                'dataset': dataset_name,
                                'file': file_path,
                                'filename': file_path.name
                            })
                    else:
                        print(f"  ‚ö†Ô∏è  {dataset_name} (–Ω–µ—Ç CSV/Excel —Ñ–∞–π–ª–æ–≤)")
    
    return datasets

def generate_insights(analysis_results):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
    print("\n" + "="*60)
    print("üéØ –ò–ù–°–ê–ô–¢–´ –î–õ–Ø –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–û–ô –ü–õ–ê–¢–§–û–†–ú–´")
    print("="*60)
    
    if not analysis_results:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_datasets = len(analysis_results)
    total_rows = sum(r['shape'][0] for r in analysis_results if r)
    total_size = sum(r['size_kb'] for r in analysis_results if r)
    
    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {total_datasets}")
    print(f"  ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {total_rows:,}")
    print(f"  ‚Ä¢ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {total_size:.1f} KB")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    categories = {}
    for result in analysis_results:
        if result:
            cat = result['name'].split('_')[0] if '_' in result['name'] else 'other'
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(result)
    
    print(f"\nüéì –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –î–õ–Ø –ü–õ–ê–¢–§–û–†–ú–´:")
    
    # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
    education_keywords = ['student', 'exam', 'performance', 'grade', 'education']
    education_datasets = [r for r in analysis_results if r and 
                         any(keyword in r['name'].lower() for keyword in education_keywords)]
    
    if education_datasets:
        print(f"\nüìö –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ ({len(education_datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤):")
        print("  ‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —É—Å–ø–µ–≤–∞–µ–º–æ—Å—Ç–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤")
        print("  ‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö")
        print("  ‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
        print("  ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ –æ–±—É—á–µ–Ω–∏–∏")
    
    # –†—ã–Ω–æ—á–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
    job_keywords = ['job', 'salary', 'skill', 'career', 'linkedin']
    job_datasets = [r for r in analysis_results if r and 
                   any(keyword in r['name'].lower() for keyword in job_keywords)]
    
    if job_datasets:
        print(f"\nüíº –†—ã–Ω–æ—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ ({len(job_datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤):")
        print("  ‚Ä¢ –ê–Ω–∞–ª–∏–∑ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –Ω–∞ —Ä—ã–Ω–∫–µ")
        print("  ‚Ä¢ –ó–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—è–º")
        print("  ‚Ä¢ –¢—Ä–µ–Ω–¥—ã –≤ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π")
        print("  ‚Ä¢ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π")
    
    print(f"\nüöÄ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø MVP:")
    print("  1. –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∫—É—Ä—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤")
    print("  2. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è")
    print("  3. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫—É—Ä—Å–∞")
    print("  4. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤")
    print("  5. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ê–ù–ê–õ–ò–ó –ó–ê–ì–†–£–ñ–ï–ù–ù–´–• –î–ê–¢–ê–°–ï–¢–û–í")
    print("="*60)
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    datasets = find_datasets()
    
    if not datasets:
        print("\n‚ùå –î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ kaggle_downloader_venv.py")
        return
    
    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(datasets)} —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    analysis_results = []
    
    for dataset_info in datasets[:10]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
        result = analyze_dataset(
            dataset_info['file'], 
            f"{dataset_info['category']}_{dataset_info['dataset']}"
        )
        if result:
            analysis_results.append(result)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
    generate_insights(analysis_results)
    
    print(f"\nüìÅ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤: {DATA_DIR.absolute()}")
    print("üìñ –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤: data/README.md")

if __name__ == "__main__":
    main() 